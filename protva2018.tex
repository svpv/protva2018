\documentclass[russian,a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{url}
\usepackage{fullpage}
\usepackage{comment}

\sloppy
\hyphenpenalty=666

\begin{document}
\title{\texttt{zrec}~--- формат\\метаданных репозитория}
\author{Алексей Турбин}
\date{17 сентября 2018 г.}
\maketitle

\begin{abstract}

\end{abstract}

\section{Введение}
Перед обновлением из репозитория скачивается большой файл с \textit{записями пакетов} (records);
в \verb|apt-rpm| это файлы \verb|pkglist.xz| и \verb|srclist.xz|, содержащие урезанные
rpm-заголовки (в \verb|repomd|-репозиториях записями можно считать сегменты XML-файла).
Файл \verb|pkglist.xz| хорошо сжат, но распаковка его занимает несколько секунд, и далее
он хранится в \verb|/var/lib/apt| в разжатом виде, занимая довольно много места (200--300 Мб;
здесь не столько жалко места на диске, сколь возникают <<тормоза>> при доступе к файлу).
Пережать же в более <<легкий>> формат \verb|pkglist| нельзя, поскольку \verb|apt| требует произвольный
доступ к записям (команда \texttt{apt-cache show} делает \verb|lseek(2)| и считывает запись).

В 2017 г.~автор предпринял попытку разработать <<легкий>> формат \verb|zpkglist|
для сжатия записей после скачивания \cite{zpkglist}.
За основу была взята библиотека сжатия \verb|LZ4|; для нее была адаптирована техника сжатия со словарем,
реализованная в библиотеке \verb|zstd|.  Перед сжатием записи группировались по 4 штуки, что значительно
улучшало коэффициент сжатия, однако произвольный доступ уже требовал распаковки группы из четырех записей.

В мае 2018 г.~Джонатан Дайетер (Jonathan Dieter) анонсировал похожий проект \verb|zchunk| \cite{dieter}
(формат файла и библиотека сжатия).  Поддержка \verb|zchunk| добавлена в \verb|dnf|, планируется к использованию
в Fedora~29 либо Fedora~30.  \verb|zchunk| основывается на алгоритме \verb|zstd|, который способен, в отличие от \verb|LZ4|,
обеспечить значительно более высокий, <<релизный>> уровень сжатия (хотя и несколько не дотягивает до \verb|xz|).
Поэтому интерес автора стал смещаться в сторону использования нового формата для сжатия на сервере (сжатый файл
не требует распаковки на клиенте).

Файл в формате \verb|zchunk| можно понимать просто как сжатый поток байтов.  При сжатии поток нарезается на куски (chunking)
и каждый кусок сжимается хотя и отдельно, но \textit{со словарем}, хранящимся в том же файле (что значительно улучшает
коэффициент сжатия).  Естественно, при сжатии записей нарезка идет по границам записей, но в одном куске может содержаться
и несколько записей.  В начале же файла хранится \textit{индекс} кусков: их размеры и хеш-суммы.  Наличие индекса делает
возможным \textit{синхронизацию}: клиент сначала скачивает индекс и потом~--- через HTTP range requests~--- недостающие
куски, перестраивая старый файл в новый.

Формат \verb|zchunk| не устраивает автора лишь в деталях.  Далее рассмотрены некоторые специальные особенности сжатия
и синхронизации, которые приводят к созданию альтернативного формата~--- \verb|zrec| (сокр.~от compressed records).

\section{О пользе сортировки}
Главным просчетом в формате \verb|zchunk| является его общность: он сжимает какие-то куски, которые распаковываются
во что угодно.  Формат \verb|zrec| вместо этого постулирует, что сжимаемые записи \textit{упорядочены} каким-либо образом
(напр., естественным образом~--- по имени пакета, но возможна и группировка по \verb|src.rpm|).
Этот постулат имеет далеко идущие последствия.

Во-первых, в отсортированном списке у записей возникает \textit{локальное сходство}, которое <<не улавливается>> словарем.
Например, пакеты \verb|perl-*| имеют между собой сходство в части зависимостей и т.\,п.  Значит, группировка соседних
записей при нарезке должна привести к значительному улучшению сжатия.  А библиотека тогда может реализовать
процедуру сжатия с автоматической группировкой.

Во-вторых, сортировка меняет требования к сопоставлению кусков.  Это даже две разные математические задачи:
1) чтобы уникально идентифицировать каждый кусок среди всех остальных, требуется 96-битный хеш (при числе кусков порядка $10^5$
и вероятности коллизии порядка $10^{-18}$); 2) чтобы определить, изменился ли кусок, требуется 60 битов хеш-материала
(т.\,к.~$2^{-60}\approx10^{-18}$).  Поэтому можно считать, что когда куски <<расставлены по местам>>, то для идентификации
куска в <<локальной окрестности>> достаточно 64-битного хеша.

\section{Группировка записей}
Группировка записей может заметно, хотя и не радикально, улучшить сжатие.  Возьмем файл \verb|srclist.xz| (1.86 Мб)~--- в разжатом
виде 10 Мб.  <<Солидное>> сжатие \verb|zstd -19| дает 1.97 Мб.  Если сжимать каждую запись по отдельности, получается 5.8 Мб~---
мало избыточности!  Радикальное улучшение дает сжатие записей со словарем~--- 2.44 Мб (словарь 512 Кб, в сжатом виде 177 Кб).
Если теперь еще группировать записи парами, получается 2.26 Мб, по три~--- 2.21 Мб, по четыре~--- 2.18 Мб и далее очень медленно.
В общем, группировка по 2-3 записи улучшает сжатие на 8-10\%, а также уменьшает в 2-3 раза размер индекса.

Записи однако нельзя группировать в ровных количествах, это приведет к \textit{сдвигу границ} и сделает невозможным синхронизацию.
Так, если из потока записей, сгруппированных парами: \texttt{AB CD EF...} будет удалена запись \verb|B|, то границы пар сдвинутся:
\texttt{AC DE F...} и все куски перестанут совпадать.  Классический подход к решению этой проблемы состоит в том, что нужно
нарезать куски переменной длины псевдослучайным образом, основываясь на данных внутри кусков (content-defined chunking).
А именно, данные сканируются скользящим окном и нарезаются, когда контрольная сумма в окне принимает определенное значение.
Упрощенная реализация такого подхода известна как rsyncable gzip.  Об оптимальной нарезке см.~статью 2005 г.~\cite{hp}
и свежую работу \cite{xia}.

Наша идея \textit{ультракороткой оптимальной нарезки} состоит в следующем: записи можно сравнивать по их хеш-коду.
Тогда если напр.~$\underline{A<B<C}>\underline{D<E}$, то упорядоченная подпоследовательность и образует группу,
а нарушение порядка дает разрез.  Нарушение порядка можно также вынужденно допустить в первых двух элементах группы.
Это приводит к следующей \textbf{Стратегии}: 1)~Поместить в очередь $A$, $B$ и $C$.  Если $B>C$, отрезать $AB$ (и тогда $C$
становится новым $A$). 2)~Иначе добавить в очередь $D$.  Если $C>D$, отрезать $ABC$.  3)~Иначе отрезать $ABCD$.

Отметим, что хотя на шагах 1) и 2) сравнение одно и то же, вероятность его разная: $\Pr(B<C)=1/2$, а $\Pr(C<D)<1/2$.
И поскольку условие одно и то же, то для преодоления рассинхронизации дается более одной возможности <<зацепиться
за нужное место>>, и в то же время рассинхронизация быстро пресекается.

Отметим также важную \textit{практическую модификацию} процедуры нарезки: для использования в качестве $A, B\ldots$
желательно хешировать не всю запись, а лишь имя пакета (или имя \verb|src.rpm| пакета без версии при группировке по \verb|src.rpm|).
Тогда изменение версии у пакета не приводит к изменению нарезки: изменение оказывается полностью локализованным внутри куска;
к рассинхронизации может привести только удаление и добавление новых пакетов.

Мы взяли файл \verb|srclist.xz| от 1 июня и 1 июля 2018 г.  Без группировки записей для регенерации нового файла
недостает 923 куска общим объемом 271 Кб, а с описанной группировкой~--- 809 кусков объемом 568 Кб.  При этом группировка
также уменьшает размер индекса примерно с 80 до 30 Кб, однако в данном случае увеличившийся объем кусков в несколько
раз перекрывает уменьшение индекса.  Видно, что при нерегулярных обновлениях группировка оказывается невыгодной.

\section{Хеширование и расстановка}
При любом изменения репозитория индекс бует скачиваться полностью, что может составлять значительную часть скачанного.
Поэтому представляет интерес дальнейшее уменьшение объема хеш-материала внутри индекса.

Выделим подзадачу: пусть каждый кусок наделяется 32-битным хешем; нужно установить соотвествие между кусками в старом
и новом файле.  Ясно, что соответствие определяется совпадением хеша, но при этом совпадения, нарушающие <<общий порядок>>,
считаются коллизиями и отсеиваются.

В общем виде эта задача известна как Longest common subsequence problem, и одним из ее решений является алгоритм Майерса
\cite{myers}, используемый в \verb|diff(1)|.  Однако алгоритм Майерса выполняется, вообще говоря, квадратичное время,
как и все остальные решения этой задаче в наиболее общем виде или в худшем случае.  Несколько точнее, сложность алгоритма Майерса~---
$O(nd)$, где $n$~--- общее число элементов в двух последовательностях, а $d$~--- число отличающихся элементов.
Увы, на больших файлах \verb|diff(1)| хорошо работает, только если изменения в них оказываются небольшими.
В нашем же случае при нерегулярных обновлениях число отличающихся кусков может быть большим.
Однако существует другой алгоритм~--- алгоритм Ханта-Шиманского со сложностью $O((r+n)\log n)$, где $r$~--- общее число
всевозможных совпадений в двух последовательностях.  Этот алгоритм деградирует, когда число неоднозначных совпадений велико
(например, из-за малой мощности алфавита, или когда в текстовых файлах много одинаковых строк).  Но в нашем случае при общем
количестве кусков порядка $2^{16}$ неоднозначными совпадениями (коллизиями 32-битного хеша) можно пренебречь, и задача расстановки
решается за $O(n\log n)$.  Позже были открыты другие алгоритмы с похожей асимптотикой, которые используются в биоинформатике
\cite[с.\,291]{seq} (в биоинформатике задача расстановки называется global alignment).  См.~тж. обзоры \cite{survey,symposium}.

Естественно, 32-битного хеша маловато для надежного сопоставления.  Поэтому формат вводит хеширование второго уровня:
пусть $h(A)$ и $h(B)$~--- хеши идущих подряд кусков, тогда в дополнение к ним вычисляется $H(A+B)$~--- 32-битный <<проверочный>>
хеш.  Важно, что функции $h$ и $H$ отличаются, т.\,е.~$H$ предоставляет \textit{дополнительную} информацию о каждом куске.
Тогда каждый кусок оказывается наделенным 64-битным хешем (поскольку с точки зрения каждого куска другой кусок можно считать
параметром).  А составная конструкция содержит 96 битов хеш-материала, что уже достаточно для уникальной идентификации.
Проверочный хеш используется на второй стадии синхронизации: если $h(A)$ и $h(B)$ совпадают, а $H(A+B)$ не совпадает, значит,
куски $A$ и $B$ надо скачать повторно.

Теперь остается только применить тот же алгоритм группировки кусков, который применяется для группировки записей:
для наделения кусков составным хешем их нужно нарезать по 2-4 штуки.  Совпадение составных кусков разбивает задачу
расстановки на мелкие подзадачи.

\section{О надежности конструкции}

\begin{thebibliography}{9}

\bibitem{zpkglist}
Alexey Tourbin. \verb|zpkglist|~--- Compressed file format\\
\url{https://github.com/svpv/zpkglist}

\bibitem{dieter}
Jonathan Dieter. What is \verb|zchunk|?\\
\url{https://www.jdieter.net/posts/2018/05/31/what-is-zchunk/}

\bibitem{hp}
Kave Eshghi, Hsiu Khuern Tang (2005).
A Framework for Analyzing and Improving Content-Based Chunking Algorithms

\bibitem{xia}
Wen Xia et al. (2016).
FastCDC: a Fast and Efficient Content-Defined Chunking Approach for Data Deduplication

\bibitem{myers}
Eugene W. Myers (1986).  An O(ND) Difference Algorithm and Its Variations

\bibitem{seq}
Dan Gusfield (1997).  Algorithms on Strings, Trees, and Sequences:
Computer Science and Computational Biology

\bibitem{survey}
L. Bergroth et al. (2000).  A Survey of Longest Common Subsequence Algorithms

\bibitem{symposium}
Mike Paterson, Vlado Dancik (1994).  Longest Common Subsequences

\end{thebibliography}

\end{document}
